; ===== vLLM (OpenAI-compatible server) =====
; Optional envs at docker run:
;   -e VLLM_MODEL="Qwen/Qwen2.5-7B-Instruct"
;   -e VLLM_PORT=8001
;   -e VLLM_TP=1
;   -e VLLM_EXTRA="--gpu-memory-utilization 0.90"
;   -e HUGGING_FACE_HUB_TOKEN=hf_xxx
;   -e HF_HOME=/home/carbon/.cache/huggingface

[program:vllm]
command=/usr/local/bin/start-vllm.sh
user=carbon
directory=/home/carbon
autostart=false
autorestart=true
startsecs=5
stdout_logfile=/var/log/vllm/vllm.out.log
stderr_logfile=/var/log/vllm/vllm.err.log
stopsignal=TERM
stopasgroup=true
killasgroup=true
priority=30
